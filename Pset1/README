Readme for problem set one

FILES INCLUDED IN THIS ASSIGNMENT
mdp.py: MDP solver for ice cream problem, uses policy and value iteration separately to find an optimal trajectory from 
        provided starting state. When run via command line, takes arguments pe for error probability, gamma for discount 
        factor, and output to determine verbosity of output ([opt] for only optimized results, and [all] for all results 
        which should only be desired for initial questions 3a-3d). Under opt option will output following figures for both 
        value iteration and policy iteration: grid map of optimal policy function, grid map of optimal value function, 
        program execution time, trajectory array, max total discounted sum of rewards, and max expected discounted sum 
        of rewards. With all option, will print previously noted results as well as answers to 3a-3d. For further detail
        look at documentaion on code
        
MDP_runner.sh: Script that runs MDP solver with varying values of pe and gamma, in order to identify how different values 
               affect optimal policy generation. Will print out results needed to answer all relevant questions in spec
               
result.txt: a text file containing all results from the script (to compare with results produced by reader)


RESPONSES TO QUESTIONS IN PSET1
Problem 0 a. This code was a collaboration between the members of team Ares (Sokchetra Eung, Victor Rios, Cooper Simpson).
          b. Resources included the class overleaf, stackexchange, and additional references listed in reference sources below.
          
Problem 1 a. The size of the state space S given L and H as the length and height of the rectangular space respectively 
             was determined to be L*H = 5*6 = 30
          b. The size of the action space A given the possible actions (movement in the four cardinal directions and 
             remaining stationary) was found to be 5
          c. See code for implementation

Problem 2 a-b. See code for implementation

Problem 3 a-e. See code for implementation
          f. Generation of the optimal policy took approximately 3.25 seconds.
          g. The expected discounted sum of rewards, given from v_pi, is 10.09 for this trajectory. The total discounted 
             sum of rewards for this trajectory is 47.83. This is expected as total doesn't take error into account like
             expected does. Look at results for trajectory
          
Problem 4 a. See code for implementation
          b. The policies generated by value iteration and policy iteration were identical. This is expected as both alogorithms
             are tasked with producing an optimal path. For this assignment there was only one optimal path for any given
             intial state, but in an excercise where more than one optimal path exists it could be expected that more than path
             could be returned. Furthermore, not only could policy and value iteration differ from each other, different runs 
             of a single algorithm could differ depending on initial conditions if more than one optimal path exists.
          c. Value iteration took about one half-second longer than policy iteration. Different machines run code at different 
             speed and although timings between machines differed, the general trend of policy finishing faster than value 
             iteration perssited regardless of machine
          
Problem 5 a. The same optimal policy was produced for both policy and value iteration in each case. 

             For the original case, the policy produced a trajectory that prioritized moving to the +10 reward space, using 
             the shortest route, despite the shortest route's nearness to the -100 reward zone. In the run with a lower 
             gamma value but the original error, the policy generated was identical to the original policy, but was 
             generated in approx. 1/5 of the time for policy itertion, and 1/6 of the time for value iteration.
             
             In the run with the lowest gamma and the original error, the policy was generated in approx. 1/12 the original 
             time for both policy and value iteration, but prioritized getting to the nearest reward. In cases where it was 
             equally near to the +1 reward and the +10 reward, the trajectory was plotted towards the +10. However in some 
             cases, where it was too close to the high negative reward zone, it decided to not move at all for fear of entering 
             the high negative reward.
             
             In the run with the original gamma and a higher error, the policy generation took around a second and a quarter 
             longer for policy iteration, and about the same time for value iteration, and was nearly identical to the original 
             save for the tendancy to stay close to corners. This makes sense, as these cases would have a higher likelyhood 
             of moving towards the desired state or not moving, given that the robot cannot move outside of the state space. 
             
             For the run with the original gamma and an even higher error, the policy generated still prioritized the 
             higher reward, but tended to move left to avoid the -100 zone where possible. In some states close to the 
             -100 zone, it chose to stay stationary rather than risk moving into the -100 zone. Policy and Value iteration 
             both produced the new policy in a similar amount of time as the original.
             
             In the run with the original gamma and the highest error, the policy generated went for the nearest reward, and 
             chose to stay still in all spaces adjacent to the -100 reward zone. Interestingly, it chose to move up in the 
             adjacent zone to the +1 reward, depending on the high error to get it to the correct location. Policy iteration 
             took approximately double the time of the original, value iteration took approximately the same amount of time.
             
             In the run with the lowest gamma and highest error, the policy generated prioritized going towards the nearest 
             reward, save for cases where it was close to the negative reward zone. Policy iteration took approx 1/10 of 
             the time of the original, and value iteration took approximately 1/12.
             
             These trends are expected because as gamma increases, you prioritize the reward near you since you only have a
             small amount of time to obtain your final sum(long distances are so far that they aren't worth it). As pe
             increases you become more wary of the street because you could easily stumble into it and pay a hefty penalty.
             Also note that as gamma becomes smaller, our algorithms end quicker and that makes sense because a smaller gamma
             causes values to converge quicker (.1^t htis zero faster than .9^t does)
             
INSTRUCTIONS/HELP     
How to run the script to produce all results:
        Requirements: python version 3.3 and above, Bash shell script to run the script.
        command Line: chmod +x MDP_runner
        command Line: ./MDP_runner
       
How to run the program with self defined inputs: gamma, pe, and ouput:
        Command Line: ./mdp.py --pe = [float between 0 and 1] --gamma = [float between 0 and 1] --output = [all | opt]
        
RESOURCES
Reference Sources:
        https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-410-principles-of-autonomy-and-decision-making-fall-2010/lecture-notes/MIT16_410F10_lec23.pdf
        https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-410-principles-of-autonomy-and-decision-making-fall-2010/lecture-notes/MIT16_410F10_lec22.pdf
        https://www.youtube.com/watch?v=Csiiv6WGzKM
        https://www.youtube.com/watch?v=4KGC_3GWuPY
        https://pymotw.com/3/argparse/
        https://towardsdatascience.com/reinforcement-learning-demystified-solving-mdps-with-dynamic-programming-b52c8093c919
