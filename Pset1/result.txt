Original Run with gamma = .9 and pe =.01 and one-time initial outputs for initial questions

Display initial policy: 
left   left   left   left   left   
left   left   left   left   left   
left   -----  -----  left   left   
left   left   left   left   left   
left   -----  -----  left   left   
left   left   left   left   left   

one-step improvement on initial policy: 
down   stay   stay   stay   left   
down   stay   stay   stay   left   
down   -----  -----  down   left   
right  right  stay   left   left   
down   -----  -----  down   left   
right  right  stay   left   left   

Policy resulting from Policy Iteration: 
down   down   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   right  right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 47.48,  42.73,  46.68,  51.67, -54.20, 
 52.81,  47.49,  51.90,  57.73, -48.76, 
 58.75,  -----   -----   64.49, -42.68, 
 65.34,  58.60,  65.44,  72.00, -35.93, 
 72.69,  -----   -----   80.37, -28.40, 
 80.84,  89.91,  100.00,  89.66, -20.06, 
CPU time for policy iteration is 3863.0493359960383 ms 
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  95.66 
 Expected Sum of Discounted Rewards:  46.675982383936486

Policy resulting from Value: 
down   down   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   right  right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 47.48,  42.73,  46.68,  51.67, -54.20, 
 52.81,  47.49,  51.90,  57.73, -48.76, 
 58.75,  -----   -----   64.49, -42.68, 
 65.34,  58.60,  65.44,  72.00, -35.93, 
 72.69,  -----   -----   80.37, -28.40, 
 80.84,  89.91,  100.00,  89.66, -20.06, 
CPU time for value iteration is 4053.401965997182 ms
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  95.66 
 Expected Sum of Discounted Rewards:  46.675982383936486
###########################################################################\n
Run with lower gamma = .5 and original pe =.01

Policy resulting from Policy Iteration: 
down   down   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   right  right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
  0.15,   0.08,   0.09,   0.06, -100.35, 
  0.30,   0.15,   0.18,   0.37, -100.19, 
  0.61,  -----   -----    0.99, -99.88, 
  1.23,   1.05,   2.11,   2.24, -99.26, 
  2.47,  -----   -----    4.76, -98.00, 
  4.96,   9.96,  20.00,   9.82, -95.49, 
CPU time for policy iteration is 656.9374749960843 ms 
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  0.31 
 Expected Sum of Discounted Rewards:  0.09070829806482354

Policy resulting from Value: 
down   down   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   right  right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
  0.15,   0.08,   0.09,   0.06, -100.35, 
  0.30,   0.15,   0.18,   0.37, -100.19, 
  0.61,  -----   -----    0.99, -99.88, 
  1.23,   1.05,   2.11,   2.24, -99.26, 
  2.47,  -----   -----    4.76, -98.00, 
  4.96,   9.96,  20.00,   9.82, -95.49, 
CPU time for value iteration is 631.1979899910511 ms
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  0.31 
 Expected Sum of Discounted Rewards:  0.09070829806482575
###########################################################################\n
Run with lowest gamma = .01 and original pe =.01

Policy resulting from Policy Iteration: 
down   left   stay   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
right  right  stay   left   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
  0.00,  0.00,  0.00,  0.00, -100.08, 
  0.00,   0.00,  0.00,  0.00, -100.08, 
  0.00,  -----   -----   0.00, -100.08, 
  0.01,   0.11,   1.11,   0.09, -100.07, 
  0.01,  -----   -----    0.08, -100.07, 
  0.11,   1.10,  11.11,   1.08, -99.97, 
CPU time for policy iteration is 254.29729399911594 ms 
Optimal Path:  [[2, 5]] 
 Sum of discounted rewards:  0.0 
 Expected Sum of Discounted Rewards:  0.0

Policy resulting from Value: 
down   left   stay   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
right  right  stay   left   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
  0.00,  0.00,  0.00,  0.00, -100.08, 
  0.00,   0.00,  0.00,  0.00, -100.08, 
  0.00,  -----   -----   0.00, -100.08, 
  0.01,   0.11,   1.11,   0.09, -100.07, 
  0.01,  -----   -----    0.08, -100.07, 
  0.11,   1.10,  11.11,   1.08, -99.97, 
CPU time for value iteration is 225.35133200290147 ms
Optimal Path:  [[2, 5]] 
 Sum of discounted rewards:  0.0 
 Expected Sum of Discounted Rewards:  0.0
###########################################################################\n
Run with original = .9 and higher error pe =.1

Policy resulting from Policy Iteration: 
down   left   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   left   right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 44.09,  39.38,  35.53,  36.94, -74.11, 
 49.52,  43.99,  39.66,  44.41, -67.56, 
 55.76,  -----   -----   53.10, -59.81, 
 62.63,  53.36,  54.41,  62.77, -51.17, 
 70.59,  -----   -----   73.88, -41.27, 
 79.28,  89.04,  100.00,  86.16, -30.57, 
CPU time for policy iteration is 4751.994295991608 ms 
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  95.66 
 Expected Sum of Discounted Rewards:  35.529434114645476

Policy resulting from Value: 
down   left   down   down   left   
down   left   right  down   left   
down   -----  -----  down   left   
down   left   right  down   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 44.09,  39.38,  35.53,  36.94, -74.11, 
 49.52,  43.99,  39.66,  44.41, -67.56, 
 55.76,  -----   -----   53.10, -59.81, 
 62.63,  53.36,  54.41,  62.77, -51.17, 
 70.59,  -----   -----   73.88, -41.27, 
 79.28,  89.04,  100.00,  86.16, -30.57, 
CPU time for value iteration is 3719.6548740030266 ms
Optimal Path:  [[2, 5], [2, 4], [3, 4], [3, 3], [3, 2], [3, 1], [3, 0], [2, 0]] 
 Sum of discounted rewards:  95.66 
 Expected Sum of Discounted Rewards:  35.529434114645476
###########################################################################\n
Run with original = .9 and even higher error pe =.5

Policy resulting from Policy Iteration: 
down   left   left   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
down   left   left   stay   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 23.20,  19.34,  14.27,  0.00, -150.94, 
 28.10,  22.28,  15.93,  0.00, -150.91, 
 35.23,  -----   -----   0.00, -150.70, 
 42.92,  25.87,  15.30,  0.00, -149.03, 
 55.51,  -----   -----   15.16, -136.11, 
 67.89,  82.44,  100.00,  51.15, -111.67, 
CPU time for policy iteration is 3322.2619050065987 ms 
Optimal Path:  [[2, 5], [1, 5], [0, 5], [0, 4], [0, 3], [0, 2], [0, 1], [0, 0], [1, 0], [2, 0]] 
 Sum of discounted rewards:  77.48 
 Expected Sum of Discounted Rewards:  14.27495308604761

Policy resulting from Value: 
down   left   left   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
down   left   left   stay   left   
down   -----  -----  down   left   
right  right  stay   left   left   

With V(s) function: 
 23.20,  19.34,  14.27,  0.00, -150.94, 
 28.10,  22.28,  15.93,  0.00, -150.91, 
 35.23,  -----   -----   0.00, -150.70, 
 42.92,  25.87,  15.30,  0.00, -149.03, 
 55.51,  -----   -----   15.16, -136.11, 
 67.89,  82.44,  100.00,  51.15, -111.67, 
CPU time for value iteration is 3742.8396300092572 ms
Optimal Path:  [[2, 5], [1, 5], [0, 5], [0, 4], [0, 3], [0, 2], [0, 1], [0, 0], [1, 0], [2, 0]] 
 Sum of discounted rewards:  77.48 
 Expected Sum of Discounted Rewards:  14.27495308604761
###########################################################################\n
Run with original = .9 and highest error pe =.9

Policy resulting from Policy Iteration: 
down   left   left   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
down   up     stay   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   

With V(s) function: 
  2.55,   1.71,   0.89,  0.00, -254.78, 
  4.01,   2.16,   1.02,  0.00, -254.78, 
  7.67,  -----   -----   0.00, -254.78, 
 12.82,   9.15,   10.00,  0.00, -254.78, 
 23.32,  -----   -----   0.00, -254.78, 
 38.55,  62.28,  100.00,  0.00, -254.78, 
CPU time for policy iteration is 6409.089242995833 ms 
Optimal Path:  [[2, 5], [1, 5], [0, 5], [0, 4], [0, 3], [0, 2], [0, 1], [0, 0], [1, 0], [2, 0]] 
 Sum of discounted rewards:  77.48 
 Expected Sum of Discounted Rewards:  0.8863754796467174

Policy resulting from Value: 
down   left   left   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
down   up     stay   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   

With V(s) function: 
  2.55,   1.71,   0.89,  0.00, -254.78, 
  4.01,   2.16,   1.02,  0.00, -254.78, 
  7.67,  -----   -----   0.00, -254.78, 
 12.82,   9.15,   10.00,  0.00, -254.78, 
 23.32,  -----   -----   0.00, -254.78, 
 38.55,  62.28,  100.00,  0.00, -254.78, 
CPU time for value iteration is 4113.60769100429 ms
Optimal Path:  [[2, 5], [1, 5], [0, 5], [0, 4], [0, 3], [0, 2], [0, 1], [0, 0], [1, 0], [2, 0]] 
 Sum of discounted rewards:  77.48 
 Expected Sum of Discounted Rewards:  0.8863754796823942
###########################################################################\n
Run with lowest gamma = .1 and highest error pe =.9

Policy resulting from Policy Iteration: 
down   left   stay   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   

With V(s) function: 
  0.00,  0.00,  0.00,  0.00, -107.24, 
  0.00,   0.00,  0.00,  0.00, -107.24, 
  0.00,  -----   -----   0.00, -107.24, 
  0.00,   0.04,   1.11,  0.00, -107.24, 
  0.00,  -----   -----   0.00, -107.24, 
  0.01,   0.38,  11.11,  0.00, -107.24, 
CPU time for policy iteration is 315.5511280056089 ms 
Optimal Path:  [[2, 5]] 
 Sum of discounted rewards:  0.0 
 Expected Sum of Discounted Rewards:  0.0

Policy resulting from Value: 
down   left   stay   stay   left   
down   left   left   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   
down   -----  -----  stay   left   
right  right  stay   stay   left   

With V(s) function: 
  0.00,  0.00,  0.00,  0.00, -107.24, 
  0.00,   0.00,  0.00,  0.00, -107.24, 
  0.00,  -----   -----   0.00, -107.24, 
  0.00,   0.04,   1.11,  0.00, -107.24, 
  0.00,  -----   -----   0.00, -107.24, 
  0.01,   0.38,  11.11,  0.00, -107.24, 
CPU time for value iteration is 221.76830199896358 ms
Optimal Path:  [[2, 5]] 
 Sum of discounted rewards:  0.0 
 Expected Sum of Discounted Rewards:  0.0
