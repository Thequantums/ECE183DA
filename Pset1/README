Readme for problem set one

mdp.py: MDP solver, uses policy and value iteration. When run via command line, takes arguments pe for global error, gamma for discount factor,
        and output to determine verbosity of output ([opt] for only optimized results, and [all] for all results). 
MDP_runner.sh: Runs MDP solver with varying values of pe and gamma, in order to identify how different values affect optimal policy generation
result.txt: a text file contains all results from the script.

Problem 0 a. This code was a collaboration between the members of team Ares (Sokchetra Eung, Victor Rios, Cooper Simpson)
          b. Resources included the class overleaf, stackexchange,
          
Problem 1 a. The size of the state space S given L and H as the length and height of the rectangular space respectively was determined
             to be L*H
          b. The size of the action space A given the possible actions (movement in the four cardinal directions and remaining stationary)
             was found to be 5
          c. See code for implementation

Problem 3 f. Generation of the optimal policy took approximately 3.25 seconds.
          g. The expected discounted sum of rewards, given from v_pi, is 10.09 for this trajectory. The total discounted sum of rewards for this trajectory is 53.14
          
Problem 4 b. The policies generated by value iteration and policy iteration were identical. 
          c. Value iteration took about one half-second longer than policy iteration.
          
Problem 5 a. The same optimal policy was produced for both policy and value iteration in each case. 

             For the original case, the policy produced a trajectory that prioritized moving to the +10 reward space, using the shortest route,
             despite the shortest route's nearness to the -100 reward zone. In the run with a lower gamma value but the original error, the policy generated was identical to the original policy, but was generated in approx. 1/5 of the time for policy itertion, and 1/6 of the time
             for value iteration.
             
             In the run with the lowest gamma and the original error, the policy was generated in approx. 1/12 the original time for both policy and value iteration, but prioritized getting to the nearest reward. In cases where it was equally near to the +1 reward and the +10 reward,
             the trajectory was plotted towards the +10. However in some cases, where it was too close to the high negative reward zone, it decided to not move at all for fear of entering the high negative reward.
             
             In the run with the original gamma and a higher error, the policy generation took around a second and a quarter longer for policy iteration, and about the same time for value iteration, 
             and was nearly identical to the original save for the tendancy to stay close to corners. This makes sense, as these cases would 
             have a higher likelyhood of moving towards the desired state or not moving, given that the robot cannot move outside of the state space. 
             
             For the run with the original gamma and an even higher error, the policy generated still prioritized the 
             higher reward, but tended to move left to avoid the -100 zone where possible. In some states close to the -100 zone, it chose to stay stationary rather than risk moving into the -100 zone.
             Policy and Value iteration both produced the new policy in a similar amount of time as the original.
             
             In the run with the original gamma and the highest error, the policy generated went for the nearest reward, and chose to stay still in all spaces adjacent to the -100 reward zone. Interestingly,
             it chose to move up in the adjacent zone to the +1 reward, depending on the high error to get it to the correct location. Policy iteration took approximately double the time of the original, value iteration took
             approximately the same amount of time.
             
             In the run with the lowest gamma and highest error, the policy generated prioritized going towards the nearest reward, save for cases where it was close to the negative reward zone.
             Policy iteration took approx 1/10 of the time of the original, and value iteration took approximately 1/12.
             
             
How to run the script to produce all results:
        Requirements: python version 3.3 and above, Bash shell script to run the script.
        command Line: chmod +x MDP_runner
        command Line: ./MDP_runner
       
How to run the program with self defined inputs: gamma and pe:
        Command Line: ./mdp.py --pe = [float] --gamma = [float]
        
Reference Sources:
        https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-410-principles-of-autonomy-and-decision-making-fall-2010/lecture-notes/MIT16_410F10_lec23.pdf
        https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-410-principles-of-autonomy-and-decision-making-fall-2010/lecture-notes/MIT16_410F10_lec22.pdf
        https://www.youtube.com/watch?v=Csiiv6WGzKM
        https://pymotw.com/3/argparse/
          
