Readme for problem set one

mdp.py: MDP solver, uses policy and value iteration. When run via command line, takes arguments pe for global error, gamma for discount factor,
        and output to determine verbosity of output ([opt] for only optimized results, and [all] for all results). 
MDP_runner.sh: Runs MDP solver with varying values of pe and gamma, in order to identify how different values affect optimal policy generation


Problem 0 a. This code was a collaboration between the members of team Ares (Sokchetra Eung, Victor Rios, Cooper Simpson)
          b. Resources included the class overleaf, stackexchange,
          
Problem 1 a. The size of the state space S given L and H as the length and height of the rectangular space respectively was determined
             to be L*H
          b. The size of the action space A given the possible actions (movement in the four cardinal directions and remaining stationary)
             was found to be 5
          c. See code for implementation

Problem 3 f. Generation of the optimal policy took approximately one second.
          g.
          
Problem 4 b. The policies generated by value iteration and policy iteration were identical. 
          c. Value iteration took a few milliseconds longer than policy iteration.
          
Problem 5 a. 
          
